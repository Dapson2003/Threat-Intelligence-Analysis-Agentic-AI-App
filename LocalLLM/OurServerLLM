Sorted LLMs â€“ Most to Least Capable
ğŸ”¢ Rank	ğŸ·ï¸ Model Name	ğŸ§  Capability Summary
1ï¸âƒ£	qwen3:235b	ğŸ’ª Top-tier, massive 235B model, general-purpose, highly capable
2ï¸âƒ£	llama4:128x17b	ğŸ§  Extremely powerful aggregate LLaMA4 model, great for broad use cases
3ï¸âƒ£	qwen2.5-coder:32b	ğŸ‘¨â€ğŸ’» Coding specialist model, optimized for programming and tool use
4ï¸âƒ£	qwen3:32b	âš–ï¸ Strong general-purpose model, lighter than 235b but very capable
5ï¸âƒ£	llama4:16x17b	âš™ï¸ High-end general reasoning, less aggregate power than 128x17b
6ï¸âƒ£	sciphi/triplex:latest	ğŸ”¬ Science/technical reasoning-focused model, niche but powerful in STEM
7ï¸âƒ£	huihui_ai/foundation-sec-abliterated:8b-fp16	ğŸ” Security-tuned model, finetuned for threat intel use
8ï¸âƒ£	jonasbg/Foundation-Sec-8B:latest	ğŸ” Similar to above, 8B finetuned for cybersecurity context
9ï¸âƒ£	huihui_ai/foundation-sec-abliterated:8b	ğŸ“‰ Non-fp16 version, slightly older/lower perf than fp16 version
ğŸ”Ÿ	granite3.2:8b	âš™ï¸ General 8B LLM (IBM Granite), basic capability
ğŸŸ¤	mxbai-embed-large:latest	â— Embedding-only model, not for generation/chat
âšª	nomic-embed-text:latest	â— Text embedder, for semantic search, not LLM